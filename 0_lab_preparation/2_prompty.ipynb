{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Prompty\n",
    "\n",
    "Microsoft Prompty is a tool designed to help developers create, manage, and evaluate prompts for LLMs more efficiently. It works within the VSCode environment and is especially useful for refining AI interactions in GenAI pplications. <br>\n",
    "Prompty provides a standardized format (using markdown) for defining prompts, making it easier to understand, share, and debug. It allows developers to quickly prototype\n",
    "\n",
    "-   Reference: https://microsoft.github.io/promptflow/tutorials/prompty-quickstart.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "\n",
    "def get_lab_prep_dir(target_dir=\"slm-innovator-lab\"):\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    if target_dir in current_dir:\n",
    "        lab_abs_dir = current_dir.split(target_dir)[0] + target_dir\n",
    "    else:\n",
    "        lab_abs_dir = current_dir \n",
    "        \n",
    "    lab_prep_dir = os.path.join(lab_abs_dir, \"0_lab_preparation\")\n",
    "    print(\"Lab prep dir: \", lab_prep_dir)    \n",
    "    return lab_prep_dir\n",
    "\n",
    "sys.path.append(os.path.abspath(get_lab_prep_dir()))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection override\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
    "\n",
    "# override configuration with AzureOpenAIModelConfiguration\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=\"${env:AZURE_OPENAI_ENDPOINT}\",  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
    "    api_key=\"${env:AZURE_OPENAI_API_KEY}\",\n",
    "    api_version=\"${env:AZURE_OPENAI_API_VERSION}\",\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import Prompty\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"./prompty/basic.prompty\", model=override_model)\n",
    "\n",
    "# execute the flow as function\n",
    "result = f(question=\"What is the capital of Seoul?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Prompty.load(source=\"./prompty/english.prompty\", model=override_model)\n",
    "\n",
    "context = \"\"\"\n",
    "The Alpine Explorer Tent features a detachable partition to ensure privacy, \n",
    "numerous mesh windows and adjustable vents for ventilation, and a waterproof design. \n",
    "It also includes a built-in gear loft for storing outdoor essentials. \n",
    "In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
    "\"\"\"\n",
    "result = f(firstName=\"Hyo\", context=context, question=\"What would you like to know about the tent?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = Prompty.load(source=\"./prompty/basic.prompty\", model=override_model)\n",
    "eval_flow = Prompty.load(\"./prompty/eval.prompty\", model=override_model)\n",
    "\n",
    "question = \"What is the capital of South Korea?\"\n",
    "ground_truth = \"Seoul\"\n",
    "\n",
    "result = flow(question=question)\n",
    "eval_result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
    "\n",
    "print(f\"result: {result}\")\n",
    "print(f\"eval_result: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"English\"\n",
    "f = Prompty.load(\"./prompty/chat.prompty\", model=override_model)\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a factual chatbot that is also sarcastic.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}\n",
    "]\n",
    "question = f\"Can you speak more scientifically in {LANGUAGE}?\"\n",
    "result = f(chat_history=chat_history, question=question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
