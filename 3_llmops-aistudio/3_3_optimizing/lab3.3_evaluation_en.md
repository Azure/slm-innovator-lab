---
layout: default
title: Lab 3.3.1 Evaluate your models using Prompt Flow to keep optimizing
permalink: /3_3_evaluation_en/
parent: Lab 3.3 Overview
grand_parent: Lab 3. LLMOps for SLM with Azure AI Studio
nav_order: 631
---

# Lab 3.3 Evaluate your models using Prompt Flow (UI)

![LLMOps](images/evaluation-monitor-flow.png)
[Evaluating and monitoring of generative AI applications](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-approach-gen-ai#evaluating-and-monitoring-of-generative-ai-applications)

### Prerequisites

- An Azure subscription where you can create an AI Hub and AI project Resource
- Deployed gpt-4o model in Azure AI Studio


### Task

- I want to quantitatively verify how well the model and RAG are answering questions 
- I want to benchmark in bulk data before production to find bottlenecks and improve 


### TOC
- 1ï¸âƒ£ Manual evaluations to review outputs of the selected model
- 2ï¸âƒ£ Conduct A/B testing with your LLM variants

- 3ï¸âƒ£ Create Automated Evaluation with variants

- 4ï¸âƒ£ Create Custom Evaluation flow on Prompt flow

### 1ï¸âƒ£ Manual evaluations to review outputs of the selected model 
1. Go to the Azure AI Studio > Tools > Evaluation
2. Click on the "Manual Evaluation" tab to create an manual evaluation to assess and compare AI application performance.
![new manual evaluation](images/new_manual_evaluation.jpg)

3. Select model you are going to test on the configurations and update the system message below. 
```
You are a math assistant, and you are going to read the context which includes simple math questions and answer with numbers only. 
```
4. Click the import test data button to import the test data. You can add your data as well if you want to test the model with the context.

5. Select the dataset you want to test on the model.
![select dataset](images/import_test_data_select_dataset.jpg)

6. Map the test data. Select question as the input and answer as the output. Click the add button to import the test data.
![map data](images/import_test_data_map_data.jpg)

7. Click the Run button to test the model with the test data. After the test is done, you can see and export the results, and you can also compare the results with the expected answers. Use thumbs up or down to evaluate the model's performance. As this result is for the manual evaluation, you can handover the result dataset to automated evaluation to evaluate the model in bulk data.
![run test](images/manual_eval_run_test.jpg)

### 2ï¸âƒ£ Conduct A/B testing with your LLM variants
Create a new chat flow with variants 
1. Azure AI Studio > Prompt flow > Click +Create to create a new flow
![create a new flow](../3_2_prototyping/images/create_new_flow.jpg)

2. In order to get a user-friendly chat interface, select Chat flow
![select Chat flow](../3_2_prototyping/images/create_new_chat_flow.jpg)

3. Put your folder name to store your Promptflow files and click the Create button
![Put your folder name](../3_2_prototyping/images/put_folder_name.jpg)

4. Change as raw file model to modify your basic chat flow
![Put your folder name](../3_2_prototyping/images/change_raw_file_mode.jpg)

5. Modify flow.dag.yaml attach the source code below. 
```
id: chat_variant_flow
name: Chat Variant Flow
inputs:
  question:
    type: string
    is_chat_input: true
  context:
    type: string
    default: >
      The Alpine Explorer Tent boasts a detachable divider for privacy, 
      numerous mesh windows and adjustable vents for ventilation, and 
      a waterproof design. It even has a built-in gear loft for storing 
      your outdoor essentials. In short, it's a blend of privacy, comfort, 
      and convenience, making it your second home in the heart of nature!
    is_chat_input: false
  firstName:
    type: string
    default: "Jake"
    is_chat_input: false
outputs:
  answer:
    type: string
    reference: ${chat_variants.output}
    is_chat_output: true
nodes:
- name: chat_variants
  type: llm
  source:
    type: code
    path: chat_variants.jinja2
  inputs:
    deployment_name: gpt-4o
    temperature: 0.7
    top_p: 1
    max_tokens: 512
    context: ${inputs.context}
    firstName: ${inputs.firstName}
    question: ${inputs.question}
  api: chat
  provider: AzureOpenAI
  connection: ''
environment:
  python_requirements_txt: requirements.txt
```
6. Change the Raw file mode again and Add the connection parameters of the LLM Node to call the deployed LLM model and Click Validate and parse input. Check inputs to the LLM Node in place.
![add the connection parameters](images/add_gpt4o_node2.jpg)

7. attach the prompt below on your chat_variants Node to request the deployed model. 

```
system:
You are an AI assistant who helps people find information. As the assistant, 
you answer questions briefly, succinctly, and in a personable manner using 
markdown and even add some personal flair with appropriate emojis.

Add a witty joke that begins with â€œBy the way,â€ or â€œBy the way. 
Don't mention the customer's name in the joke portion of your answer. 
The joke should be related to the specific question asked.
For example, if the question is about tents, the joke should be specifically related to tents.

Respond in your language with a JSON object like this.
{
  â€œanswer": 
  â€œjoke":
}

# Customer
You are helping {{firstName}} to find answers to their questions.
Use their name to address them in your responses.

# Context
Use the following context to provide a more personalized response to {{firstName}}:
{{context}}

user:
{{question}}
```

8. Save your modified flow. Make sure that your compute instance is running to execute the updated chat flow
![create a new flow](../3_2_prototyping/images/save_and_run_compute_session.jpg)

9. Let's test the current flow on the chat window
![test the flow](images/test_current_flow.jpg)

10. Now you can generate a variant and compare the results with the prompt written in Korean. Click the generate variant button to create a new variant.
![add variants](images/add_variants.jpg)

11. Add the variant name and the prompt in Korean below. Click the save button to save the variant.

```
system:
ë‹¹ì‹ ì€ ì‚¬ëžŒë“¤ì´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìžˆë„ë¡ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ 
ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ê°„ê²°í•˜ê²Œ, ê·¸ë¦¬ê³  ê°œì„± ìžˆëŠ” ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê³  
ë§ˆí¬ë‹¤ìš´ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ê³  ì ì ˆí•œ ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ê°œì¸ì ì¸ ê°ê°ì„ ë”í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.

"ê·¸ëŸ°ë°, "ë¡œ ì‹œìž‘í•˜ëŠ” ìž¬ì¹˜ ìžˆëŠ” ë†ë‹´ì„ ì¶”ê°€í•˜ì„¸ìš”. ë‹µë³€ì˜ ë†ë‹´ ë¶€ë¶„ì—ì„œëŠ” ê³ ê°ì˜ ì´ë¦„ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. 
ë†ë‹´ì€ ì§ˆë¬¸í•œ íŠ¹ì • ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ í…íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì¸ ê²½ìš° ë†ë‹´ì€ í…íŠ¸ì™€ êµ¬ì²´ì ìœ¼ë¡œ ê´€ë ¨ëœ ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ json ê°ì²´ë¡œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.
{
  "answer": 
  "joke":
}

# Customer
ë‹¹ì‹ ì€ {{firstName}} ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ë„ë¡ ë•ê³  ìžˆìŠµë‹ˆë‹¤.
ë‹µë³€ì— ìƒëŒ€ë°©ì˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ìƒëŒ€ë°©ì„ ì–¸ê¸‰í•˜ì„¸ìš”. 

# Context
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ {{firstName}}ì—ê²Œ ë³´ë‹¤ ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€ ë°”ëžë‹ˆë‹¤:
{{context}}

user:
{{question}}
```

12. Now you can test the variants on the chat window setting one of variants as default. Click the Run button to test the variant. 


### 3ï¸âƒ£ Create QnA Relevance Evaluation flow with variants
1. Go to the Azure AI Studio > Tools > Evaluation

2. Click on the "+New Evaluation" on the Automated evaluations tab to create. 

3. Click on the "Prompt flow" to select a flow to evaluaute its output
![new evaluation](images/new_promptflow_evaluation.jpg)

4. Add basic information for the evaluation. Put the name of the evaluation as 'variant1_en' and select the flow you want to evaluate. Select "Question and answer with context" as your evaluation scenario. Click the Next button to continue.
![basic information](images/evaluation_basic_info.jpg)

5. Add 'simple_qna_data_en.jsonl' as your dataset and map the question, firstName, context and dataset column. Click the Next button to continue.
![map data](images/evaluation_map_data.jpg)

6. Select Evaluation Metrics. You can select the metrics against which you want to evaluate the model. Enter the connection and deployment model and click the Next button, then review the final configuration and click the Submit button to start/wait for the evaluation.
![select metrics](images/evaluation_select_metrics.jpg)
![running evaluation](images/evaluation_running.jpg)

> ðŸ§ª +For Your Information<br>
Evaluator is an asset that can be used to run evaluation. You can define evaluator in SDK and run evaluation to generate scores of one or more metrics. In order to use AI-assisted quality and safety evaluators with the prompt flow SDK, check the [Evaluate with the prompt flow SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk)  
