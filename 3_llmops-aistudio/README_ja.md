---
layout: default
title: Lab 3. LLMOps for SLM with Azure AI Studio
nav_order: 6
has_children: true
---

[English](README.md)

# Lab 3. LLMOps for SLM with Azure AI Studio

この E2E の例は、Azure Open AI を採用したばかりで、品質保証のための Promptflow を使用して LLM 評価パイプラインをゼロから構築したいユーザー向けです。実験、モデル品質評価、デプロイ、および微調整された LLM 後の Prompt flow やその他のツールを使用したパフォーマンス監視のエンドツーエンドのプロセスを紹介します。

## 概要

このラボでは、現在のユースケースに従って、前のラボで微調整したモデルのセットアップ、テスト、デプロイ、評価、監視の方法を学習します。Azure AI Studio と Prompt フローを活用することで、カスタム AI モデルをデプロイおよび利用するための LLMOps パイプラインを確立します。この E2E の例は、現在の状況に基づいて 5 つのシナリオに分かれています。

**シナリオ 1: LLMOps 用に Azure AI Studio を設定する**

**シナリオ 2: Promptflow を使用した第 1 世代 AI アプリの基本的な LLMOps**

**シナリオ 3: プロンプト フローを使用してモデルを評価し、最適化を続ける**

**シナリオ 4: 運用前に Azure AI Studio を使用したコンテンツの安全性** 

[//]: # (**シナリオ 5: 信頼性と監視のために Azure API Management と Azure Monitor をアタッチする (未定)**)

[//]: # (**シナリオ 6: サービスのベンチマークと最適化**)

[//]: # (**ボーナストラック:プロンプトフローのベストプラクティス**)

[//]: # (**TODO:Jekyllを使用して韓国語のすべてのコンテンツを自動的に翻訳します**)

[//]: # (**ボーナストラック:CLIバージョンのPromptフローを使用してローカルLangChainプロジェクトを作成**)

## ️リソースをクリーンアップする
未定

## 参考
[LLMOpsプロンプトフローテンプレートgithub](https://github.com/microsoft/llmops-promptflow-template) 

[GenAIOpsのgithub](https://github.com/Azure/GenAIOps)

[Phi-3クックブック](https://github.com/microsoft/Phi-3CookBook?wt.mc_id=studentamb_279723)

https://github.com/just-the-docs/just-the-docs?tab=readme-ov-file#user-content-fn-2-6204df4f8c0dad5766232d4558ca98cf 

https://serverspace.io/support/help/install-ruby-on-rails-ubuntu-20-04/ 

https://jekyllrb.com/ 

[//]: # (バンドルエグゼクティブジキルサーブ、Ctrl + C)